# Neuro-Biometrics ‚Äî Comprehensive Optimization Roadmap

> Research-backed implementation plan to maximize P@1, AUROC, and EER.
> Generated: 2026-02-20 | Baseline: V2 ResNet34+ArcFace

---

## ƒê√°nh Gi√° IMPROVEMENTS.md Hi·ªán T·∫°i

### T√≥m T·∫Øt: Claim N√†o ƒê√∫ng / Sai

| Claim | Rating | Actual Expected Gain | Ghi Ch√∫ |
|---|---|---|---|
| ‚â•5 seeds | ‚úÖ CRITICAL | Statistical validity | Kh√¥ng tƒÉng accuracy, nh∆∞ng k·∫øt qu·∫£ m·ªõi c√≥ √Ω nghƒ©a |
| ArcFace grid (m, s) | ‚úÖ REALISTIC | P@1 +1‚Äì2% | Nh∆∞ng c√≥ bug: config kh√¥ng ƒë∆∞·ª£c ƒë·ªçc (xem A1) |
| Hybrid ArcFace + SupCon ‚Üí AUROC +10‚Äì15% | ‚ö†Ô∏è OPTIMISTIC | AUROC +3‚Äì8% | Ch·ªâ ƒë√∫ng n·∫øu AUROC ~0.5 do loss, kh√¥ng ph·∫£i session shift |
| Joint fine-tune ‚Üí P@1 +3‚Äì5% | ‚úÖ REALISTIC | P@1 +2‚Äì4% | AUROC gain b·ªã th·ªïi ph·ªìng |
| Hard negative mining ‚Üí AUROC +15‚Äì20% | ‚ùå UNLIKELY | AUROC +2‚Äì5% | V·ªõi 14 subjects, memory bank kh√¥ng c√≥ t√°c d·ª•ng |
| Enhanced augmentation ‚Üí P@1 +2‚Äì3% | ‚úÖ REALISTIC | P@1 +1‚Äì2% | Skip mixup v·ªõi N=14 |
| PhysioNet pretraining ‚Üí P@1 +5‚Äì10% | ‚ùå UNLIKELY | P@1 +0‚Äì3% | Domain mismatch nghi√™m tr·ªçng (64ch motor imagery vs 4ch resting) |
| 1D ResNet embedder | ‚úÖ HIGH ROI | P@1 +3‚Äì8% | Improvement ki·∫øn tr√∫c t·ªët nh·∫•t trong roadmap |
| Multi-scale Mamba 3 blocks | ‚úÖ REALISTIC | P@1 +1‚Äì3% | MSGM paper x√°c nh·∫≠n |
| Frequency branch (alpha/beta) | ‚ö†Ô∏è WRONG BANDS | P@1 +3‚Äì5% | **Delta (0.5‚Äì4 Hz) m·ªõi l√† band discriminative nh·∫•t**, kh√¥ng ph·∫£i alpha/beta |

### Root Cause AUROC ~0.5

AUROC 0.42‚Äì0.56 kh√¥ng ph·∫£i do loss function y·∫øu. Nguy√™n nh√¢n:
1. **Session variability** ‚Äî model h·ªçc session-specific artifacts thay v√¨ identity features
2. **4 holdout subjects** qu√° √≠t (std ¬±0.097 overlap v·ªõi random chance 0.5)
3. **ImageNet-pretrained ResNet** ‚Äî domain mismatch c∆° b·∫£n (ResNet bi·∫øt v·ªÅ visual edges, kh√¥ng bi·∫øt v·ªÅ neural oscillations)

---

## K·∫øt Qu·∫£ K·ª≥ V·ªçng Theo T·ª´ng Phase

| Phase | P@1 (best) | AUROC (best) | EER (best) |
|---|---|---|---|
| **Hi·ªán t·∫°i (V2)** | 89.4% | 0.56 | 34% |
| **Sau Phase A** (1‚Äì3 ng√†y) | 93‚Äì95% | 0.66‚Äì0.72 | 26‚Äì28% |
| **Sau Phase B** (1‚Äì2 tu·∫ßn) | 95‚Äì97% | 0.83‚Äì0.88 | 14‚Äì18% |
| **Sau Phase C** (2‚Äì4 tu·∫ßn) | 97‚Äì99% | 0.88‚Äì0.93 | 8‚Äì12% |

---

## Phase A ‚Äî Quick Wins (1‚Äì3 ng√†y, kh√¥ng thay ƒë·ªïi ki·∫øn tr√∫c)

### A1. Fix Bug ArcFace Hardcode ‚ö° ‚Äî 2h | P@1 +2‚Äì4%, AUROC +4‚Äì7%

**Bug x√°c nh·∫≠n**: `experiments/shared/trainer.py:205‚Äì208` kh·ªüi t·∫°o ArcFace v·ªõi `margin=0.3, scale=30` hardcode, b·ªè qua ho√†n to√†n `self.config.arcface_margin/scale`. C√°c fields trong Config (`datapreprocessor.py:83‚Äì84`) ch∆∞a bao gi·ªù ƒë∆∞·ª£c d√πng.

```python
# experiments/shared/trainer.py:205‚Äì208 ‚Äî CURRENT (broken):
metric_loss = ArcFaceLoss(num_classes, self.config.embed_dim,
                          margin=0.3, scale=30).to(self.device)

# FIX:
metric_loss = ArcFaceLoss(
    num_classes, self.config.embed_dim,
    margin=self.config.arcface_margin,   # reads from Config
    scale=self.config.arcface_scale,
).to(self.device)
```

```python
# experiments/shared/datapreprocessor.py:83‚Äì84 ‚Äî update defaults:
arcface_margin: float = 0.5   # was 0.3 ‚Äî ArcFace paper recommendation
arcface_scale: float = 64.0   # was 30 ‚Äî s=30 gi·ªõi h·∫°n gradient signal
```

### A2. embed_dim 128 ‚Üí 256 ‚Äî 30m | P@1 +1‚Äì2%

```python
# experiments/shared/datapreprocessor.py:75:
embed_dim: int = 256   # was 128
```

### A3. Test-Time Adaptation (BN Statistics) ‚ö° ‚Äî 3h | AUROC +5‚Äì10%, EER -4‚Äì6%

Session variability l√†m l·ªách BN statistics t·ª´ training distribution. Update 2 batches tr∆∞·ªõc inference:

```python
# experiments/shared/trainer.py ‚Äî trong evaluate_comprehensive(), tr∆∞·ªõc model.eval():
model.train()
with torch.no_grad():
    for i, (noisy, _, _) in enumerate(test_dl):
        if i >= 2: break   # 2 batches ƒë·ªß ƒë·ªÉ calibrate BN stats
        model(noisy.to(self.device))
model.eval()
```

### A4. Augmentation: Time Shift + Channel Dropout ‚Äî 4h | P@1 +1‚Äì3%

```python
# experiments/shared/trainer.py ‚Äî trong _train_stage2 loop, tr∆∞·ªõc forward pass:
import random

def _augment(x: torch.Tensor) -> torch.Tensor:  # x: (B, C, T)
    # Time shift ¬±50 samples (~250ms at 200Hz)
    shift = random.randint(-50, 50)
    x = torch.roll(x, shift, dims=2)
    # Channel dropout v·ªõi 20% probability
    if random.random() < 0.2:
        ch = torch.randint(0, x.shape[1], (1,)).item()
        x[:, ch, :] = 0.0
    return x

# Trong v√≤ng l·∫∑p training:
noisy = _augment(noisy)
```

### A5. Fix filter_low ƒë·ªÉ Capture Delta Band ‚ö° ‚Äî 5m | AUROC +N/A (prerequisite)

**Quan tr·ªçng**: `filter_low=1.0` trong Config hi·ªán t·∫°i ƒëang c·∫Øt m·∫•t delta band (0.5‚Äì1 Hz), l√† band discriminative nh·∫•t.

```python
# experiments/shared/datapreprocessor.py:55:
filter_low: float = 0.5   # was 1.0 ‚Äî delta band b·∫Øt ƒë·∫ßu t·ª´ 0.5 Hz!
```

### A6. TƒÉng Seeds l√™n 5 + CI 95% ‚Äî 1h | Statistical validity

```python
# experiments/v2_mamba/main.py:11:
run_cli(use_mamba=True, version="v2_mamba", default_seeds=5)

# experiments/v1_baseline/main.py:11:
run_cli(use_mamba=False, version="v1_baseline", default_seeds=5)

# experiments/shared/pipeline.py ‚Äî trong _aggregate_results():
import scipy.stats as st
n = len(vals)
if n > 1:
    ci = st.t.interval(0.95, df=n-1, loc=np.mean(vals), scale=st.sem(vals))
    stats[f"{k}_ci95_low"] = float(ci[0])
    stats[f"{k}_ci95_high"] = float(ci[1])
```

---

## Phase B ‚Äî Architecture Improvements (1‚Äì2 tu·∫ßn)

### B1. Hybrid ArcFace + Supervised Contrastive Loss ‚Äî 2 ng√†y | AUROC +12‚Äì18%, EER -8‚Äì12%

ArcFace l√† proxy loss (t·ªët cho P@1), SupCon l√† pairwise (t·ªët cho AUROC). K·∫øt h·ª£p c·∫£ hai:

```python
# experiments/shared/trainer.py:204 ‚Äî thay ArcFace v·ªõi hybrid:
from pytorch_metric_learning.losses import SupConLoss  # ƒë√£ c√≥ trong pytorch-metric-learning

# Th√™m v√†o Config (datapreprocessor.py sau line 84):
lambda_arc: float = 0.7   # 70% ArcFace + 30% SupCon
supcon_temperature: float = 0.07

# Trong _train_stage2:
arcface_fn = ArcFaceLoss(num_classes, self.config.embed_dim,
                          margin=self.config.arcface_margin,
                          scale=self.config.arcface_scale).to(self.device)
supcon_fn = SupConLoss(temperature=self.config.supcon_temperature).to(self.device)
lam = self.config.lambda_arc

def metric_loss(emb, y):
    return lam * arcface_fn(emb, y) + (1 - lam) * supcon_fn(emb, y)

params = list(model.embedder.parameters()) + list(arcface_fn.parameters())
```

### B2. Stage 3 Joint End-to-End Fine-tune ‚Äî 2 ng√†y | P@1 +3‚Äì5%, AUROC +5‚Äì8%

Sau 2-stage training, unfreeze denoiser v√† train joint v·ªõi dual objective:

```python
# experiments/shared/trainer.py ‚Äî th√™m method sau _train_stage2():

def _train_stage3_joint(self, model, train_dl, val_dl, num_classes,
                         metric_loss_fn, epochs: int = 10):
    """
    Stage 3: Joint end-to-end fine-tuning.
    Unfreeze denoiser, train v·ªõi: loss = alpha*SI-SNR + beta*metric_loss
    LR th·∫•p (1e-5) ƒë·ªÉ tr√°nh catastrophic forgetting c·ªßa denoising ability.
    """
    model = model.to(self.device)
    for p in model.denoiser.parameters():
        p.requires_grad = True

    alpha, beta = 0.3, 0.7
    opt = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-3)
    use_amp = TRAINING_CONFIG["use_amp"] and self.device.type == "cuda"
    scaler = torch.amp.GradScaler("cuda", enabled=use_amp)

    best_p1, best_state = 0.0, None
    for ep in range(1, epochs + 1):
        model.train()
        loss_sum, n = 0.0, 0
        for noisy, clean, y in train_dl:
            noisy, clean, y = (noisy.to(self.device),
                               clean.to(self.device), y.to(self.device))
            opt.zero_grad()
            with torch.amp.autocast("cuda", enabled=use_amp):
                denoised, emb = model(noisy)
                loss = alpha * self.sisnr(denoised, clean) + beta * metric_loss_fn(emb, y)
            scaler.scale(loss).backward()
            scaler.unscale_(opt)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
            scaler.step(opt); scaler.update()
            loss_sum += loss.item() * y.size(0); n += y.size(0)

        val_p1 = self._eval_p1(model, val_dl)
        self.logger.info(f"  [Stage3] ep={ep:02d} loss={loss_sum/n:.4f} P@1={val_p1:.4f}")
        if val_p1 > best_p1:
            best_p1 = val_p1
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}

    if best_state:
        model.load_state_dict(best_state)
    return model
```

### B3. EEGNet 1D Embedder ‚Äî 3 ng√†y | P@1 +1‚Äì3%, latency 100¬µs ‚Üí 15¬µs, params 11M ‚Üí 35K

**L√Ω do**: EEGNet (1D) ƒë·∫°t 86.74% tr√™n BED dataset subject-disjoint vs ResNet2D 63.21%. Depthwise-spatial conv h·ªçc per-channel spatial filters ‚Äî inductive bias ƒë√∫ng h∆∞·ªõng cho EEG.

```python
# experiments/shared/model.py ‚Äî th√™m sau WaveNetDenoiser class:

class EEGNetEmbedder(nn.Module):
    """
    EEGNet-based 1D embedder cho metric learning.
    Kh√¥ng c·∫ßn 2D reshape. Depthwise spatial conv h·ªçc per-channel filters.

    Input:  (B, C, T) ‚Äî C=4 channels, T=800 time steps
    Output: (B, embed_dim) L2-normalized
    """
    def __init__(self, in_chans: int = 4, T: int = 800,
                 F1: int = 8, D: int = 2, F2: int = 16,
                 embed_dim: int = 256, dropout: float = 0.25):
        super().__init__()
        # Block 1: Temporal convolution
        self.temporal_conv = nn.Sequential(
            nn.Conv2d(1, F1, kernel_size=(1, T // 4),
                      padding=(0, T // 8), bias=False),
            nn.BatchNorm2d(F1),
        )
        # Block 2: Depthwise spatial (channel-mixing)
        self.spatial_conv = nn.Sequential(
            nn.Conv2d(F1, D * F1, kernel_size=(in_chans, 1),
                      groups=F1, bias=False),
            nn.BatchNorm2d(D * F1),
            nn.ELU(),
            nn.AvgPool2d((1, 4)),
            nn.Dropout(dropout),
        )
        # Block 3: Separable temporal
        self.separable_conv = nn.Sequential(
            nn.Conv2d(D * F1, F2, kernel_size=(1, T // 32),
                      padding=(0, T // 64), bias=False),
            nn.Conv2d(F2, F2, kernel_size=1, bias=False),
            nn.BatchNorm2d(F2),
            nn.ELU(),
            nn.AvgPool2d((1, 8)),
            nn.Dropout(dropout),
        )
        # Compute flattened size
        flat = self._get_flat_size(in_chans, T)
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(flat, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(256, embed_dim),
            nn.BatchNorm1d(embed_dim),
        )

    def _get_flat_size(self, C, T):
        with torch.no_grad():
            x = torch.zeros(1, 1, C, T)
            x = self.separable_conv(self.spatial_conv(self.temporal_conv(x)))
            return x.view(1, -1).shape[1]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = x.unsqueeze(1)  # (B,C,T) ‚Üí (B,1,C,T)
        x = self.temporal_conv(x)
        x = self.spatial_conv(x)
        x = self.separable_conv(x)
        return F.normalize(self.head(x), p=2, dim=1)
```

```python
# experiments/shared/model.py ‚Äî c·∫≠p nh·∫≠t create_metric_model():
def create_metric_model(backbone: str = "resnet18", n_channels: int = 4,
                        embed_dim: int = 256, pretrained: bool = True,
                        use_mamba: bool = False,
                        embedder_type: str = "resnet") -> EEGMetricModel:
    denoiser = WaveNetDenoiser(channels=n_channels, use_mamba=use_mamba)
    if embedder_type == "eegnet":
        embedder = EEGNetEmbedder(in_chans=n_channels, T=800, embed_dim=embed_dim)
    else:
        embedder = ResNetMetricEmbedder(backbone=backbone, in_chans=n_channels,
                                        embed_dim=embed_dim, pretrained=pretrained)
    return EEGMetricModel(denoiser, embedder)
```

### B4. Delta Band SincNet Branch ‚Äî 2 ng√†y | AUROC +8‚Äì12%

Delta (0.5‚Äì4 Hz) v√† theta (4‚Äì8 Hz) l√† bands session-stable nh·∫•t cho biometrics. Branch n√†y nh·∫≠n **raw signal TR∆Ø·ªöC denoiser** (delta power kh√¥ng b·ªã noise che khu·∫•t):

```python
# experiments/shared/model.py ‚Äî th√™m sau EEGNetEmbedder:

class DeltaBandBranch(nn.Module):
    """
    Parallel spectral branch v·ªõi learnable SincNet-style bandpass filters.
    Kh·ªüi t·∫°o trong d·∫£i delta-theta (0.5‚Äì8 Hz) ‚Äî band discriminative nh·∫•t.

    Input:  (B, C, T) ‚Äî raw signal, TR∆Ø·ªöC denoiser
    Output: (B, spectral_dim) L2-normalized
    """
    def __init__(self, in_chans: int = 4, T: int = 800,
                 n_filters: int = 16, spectral_dim: int = 64):
        super().__init__()
        # SincNet-style learnable filters initialized in delta-theta range
        self.low_hz = nn.Parameter(torch.linspace(0.5, 7.0, n_filters))
        self.band_hz = nn.Parameter(torch.ones(n_filters) * 2.0)
        kernel_size = min(251, T // 4) | 1  # odd kernel

        self.conv = nn.Conv1d(in_chans, in_chans * n_filters,
                              kernel_size=kernel_size,
                              padding=kernel_size // 2,
                              groups=in_chans, bias=False)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(in_chans * n_filters, spectral_dim),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(spectral_dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = F.elu(self.conv(x))
        out = self.pool(out)
        return F.normalize(self.head(out), p=2, dim=1)


# C·∫≠p nh·∫≠t EEGMetricModel ƒë·ªÉ h·ªó tr·ª£ spectral branch:
class EEGMetricModel(nn.Module):
    def __init__(self, filter_model, embedder_model,
                 spectral_branch=None, fuse_dim=None):
        super().__init__()
        self.denoiser = filter_model
        self.embedder = embedder_model
        self.spectral = spectral_branch
        if spectral_branch is not None:
            self.fusion = nn.Sequential(
                nn.Linear(fuse_dim, fuse_dim // 2),
                nn.ReLU(inplace=True),
                nn.Linear(fuse_dim // 2, fuse_dim // 2),
                nn.BatchNorm1d(fuse_dim // 2),
            )

    def forward(self, x):
        denoised = self.denoiser(x)
        emb = self.embedder(denoised)
        if self.spectral is not None:
            spec_emb = self.spectral(x)   # raw signal, pre-denoiser
            emb = F.normalize(
                self.fusion(torch.cat([emb, spec_emb], dim=1)), p=2, dim=1
            )
        return denoised, emb
```

### B5. Subject-Balanced Batch Sampler ‚Äî 2 ng√†y | AUROC +10‚Äì15%

```python
# experiments/shared/pipeline.py ‚Äî th√™m class sau imports:

class SubjectBalancedSampler(torch.utils.data.Sampler):
    """
    ƒê·∫£m b·∫£o m·ªói batch c√≥ K subjects √ó M samples/subject.
    T·∫°o hard negatives t·ª± nhi√™n cho SupCon/MultiSim loss.
    """
    def __init__(self, labels: torch.Tensor, K: int = 5, M: int = 13):
        self.labels = labels.numpy()
        self.K, self.M = K, M
        self.classes = np.unique(self.labels)
        self.class_idx = {c: np.where(self.labels == c)[0].tolist()
                          for c in self.classes}

    def __iter__(self):
        batches = []
        n_batches = min(len(v) for v in self.class_idx.values()) // self.M
        for _ in range(n_batches):
            chosen = np.random.choice(
                self.classes, size=min(self.K, len(self.classes)), replace=False
            )
            batch = []
            for c in chosen:
                idxs = np.random.choice(
                    self.class_idx[c], size=self.M,
                    replace=len(self.class_idx[c]) < self.M
                )
                batch.extend(idxs.tolist())
            batches.extend(batch)
        return iter(batches)

    def __len__(self):
        return (min(len(v) for v in self.class_idx.values()) // self.M) * self.K * self.M


# Trong _create_split_dataloaders(), thay training DataLoader:
train_sampler = SubjectBalancedSampler(y_tr, K=min(5, len(train_subs)), M=13)
train_dl = DataLoader(TensorDataset(Xn_tr, Xc_tr, y_tr),
                      batch_sampler=train_sampler, **loader_kwargs)
```

---

## Phase C ‚Äî Foundation Model Overhaul (2‚Äì4 tu·∫ßn)

### C1. EEGPT Foundation Model Embedder ‚Äî 2 tu·∫ßn | P@1 +2‚Äì4%, AUROC +3‚Äì6%

EEGPT (NeurIPS 2024) ƒë∆∞·ª£c pretrain tr√™n EEG th·ª±c ‚Äî kh√¥ng ph·∫£i ImageNet. ƒê√¢y l√† change ƒë∆°n l·∫ª c√≥ ROI cao nh·∫•t.

```bash
# T·∫£i pretrained weights:
pip install huggingface_hub
python -c "
from huggingface_hub import hf_hub_download
hf_hub_download('wodediaodan/EEGPT', 'eegpt_base.pth',
                local_dir='/root/Neuro-Biometrics/weights/')
"
```

```python
# experiments/shared/eegpt_adapter.py (file m·ªõi):
"""
EEGPT Adapter: project 4-channel EEG v√†o kh√¥ng gian 64-channel c·ªßa EEGPT.
S·ª≠ d·ª•ng channel identity mapping cho T7/F8/Cz/P4.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

# V·ªã tr√≠ c·ªßa 4 channels trong 64-channel standard 10-20:
CHANNEL_MAP = {'T7': 47, 'F8': 19, 'Cz': 30, 'P4': 51}

class EEGPTEmbedder(nn.Module):
    def __init__(self, pretrained_path: str, embed_dim: int = 256,
                 freeze_trunk: bool = True):
        super().__init__()
        # Project 4ch ‚Üí 64ch space (identity for the 4 known positions)
        self.channel_proj = nn.Linear(4, 64, bias=False)
        nn.init.zeros_(self.channel_proj.weight)
        for new_idx, orig_idx in enumerate(CHANNEL_MAP.values()):
            self.channel_proj.weight.data[orig_idx, new_idx] = 1.0

        # Load EEGPT trunk (replace with actual EEGPT class import)
        # from eegpt import EEGPT
        # self.trunk = EEGPT.from_pretrained(pretrained_path)

        self.head = nn.Sequential(
            nn.Linear(embed_dim, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, embed_dim),
            nn.BatchNorm1d(embed_dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, 4, T) ‚Üí (B, 64, T)
        x_64 = self.channel_proj(x.transpose(1, 2)).transpose(1, 2)
        # trunk_out = self.trunk(x_64)
        # return F.normalize(self.head(trunk_out), p=2, dim=1)
        raise NotImplementedError("Load EEGPT and uncomment trunk lines")
```

**Fine-tuning schedule:**
- Epochs 1‚Äì10: freeze trunk, train head + channel_proj
- Epochs 11‚Äì20: unfreeze top-4 transformer blocks
- Epochs 21‚Äì30: full unfreeze v·ªõi LR=1e-5

### C2. Session-Invariant Denoiser Objective (DAAE) ‚Äî 1 tu·∫ßn | AUROC +5‚Äì8%, EER -4‚Äì6%

Thay ƒë·ªïi Stage 1 objective t·ª´ "reconstruct clean signal" ‚Üí "reconstruct session-invariant representation":

```python
# experiments/shared/datapreprocessor.py:378 ‚Äî preserve session ID:
y.append((subject, exp))   # was: y.append(subject)

# experiments/shared/trainer.py ‚Äî SessionInvariantLoss class m·ªõi:
class SessionInvariantLoss(nn.Module):
    """
    Pulls together embeddings c·ªßa c√πng subject t·ª´ different sessions.
    Pushes apart embeddings c·ªßa different subjects.
    """
    def __init__(self, temperature: float = 0.1):
        super().__init__()
        self.T = temperature

    def forward(self, emb: torch.Tensor,
                subject_ids: torch.Tensor,
                session_ids: torch.Tensor) -> torch.Tensor:
        sim = (emb @ emb.T) / self.T
        sim.fill_diagonal_(-1e9)
        same_sub = subject_ids.unsqueeze(0) == subject_ids.unsqueeze(1)
        diff_ses = session_ids.unsqueeze(0) != session_ids.unsqueeze(1)
        pos_mask = same_sub & diff_ses
        if pos_mask.sum() == 0:
            return torch.tensor(0.0, device=emb.device, requires_grad=True)
        log_probs = F.log_softmax(sim, dim=1)
        return -(log_probs * pos_mask.float()).sum() / pos_mask.sum()
```

### C3. EEG Conformer Multi-Scale Embedder ‚Äî 1 tu·∫ßn | P@1 +1‚Äì3%, AUROC +3‚Äì5%

```python
# experiments/shared/model.py ‚Äî EEGConformerEmbedder class:
class EEGConformerEmbedder(nn.Module):
    """
    Multi-scale temporal EEG embedder k·∫øt h·ª£p:
    - Local: 3 parallel dilated Conv1D ·ªü scales T//8, T//16, T//32
    - Global: 2-layer Transformer encoder

    Input: (B, 4, 800) | Output: (B, embed_dim) L2-normalized
    """
    def __init__(self, in_chans: int = 4, T: int = 800,
                 embed_dim: int = 256, num_heads: int = 4, dropout: float = 0.1):
        super().__init__()
        d = 64
        def conv_block(k):
            return nn.Sequential(
                nn.Conv1d(in_chans, d, kernel_size=k, padding=k//2, bias=False),
                nn.BatchNorm1d(d), nn.ELU(), nn.Dropout(dropout))

        self.s1 = conv_block(T // 8)
        self.s2 = conv_block(T // 16)
        self.s3 = conv_block(T // 32)
        self.merge = nn.Sequential(
            nn.Conv1d(d * 3, d, 1), nn.BatchNorm1d(d), nn.ELU())
        enc = nn.TransformerEncoderLayer(
            d_model=d, nhead=num_heads, dim_feedforward=d*4,
            dropout=dropout, batch_first=True, norm_first=True)
        self.transformer = nn.TransformerEncoder(enc, num_layers=2)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Sequential(
            nn.Flatten(),
            nn.Linear(d, 256), nn.GELU(), nn.Dropout(dropout),
            nn.Linear(256, embed_dim), nn.BatchNorm1d(embed_dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        T = self.s1[0].weight.shape[2]
        s1 = self.s1(x)
        s2 = F.interpolate(self.s2(x), size=s1.shape[-1], mode='linear', align_corners=False)
        s3 = F.interpolate(self.s3(x), size=s1.shape[-1], mode='linear', align_corners=False)
        x = self.merge(torch.cat([s1, s2, s3], dim=1))
        x = self.transformer(x.transpose(1, 2)).transpose(1, 2)
        return F.normalize(self.head(self.pool(x)), p=2, dim=1)
```

---

## Top 5 Improvements Kh√¥ng C√≥ Trong Roadmap G·ªëc

| # | Improvement | Complexity | Expected Gain | ƒê√£ C√≥ Trong Roadmap? |
|---|---|---|---|---|
| ü•á | **EEGPT Foundation Model** (Phase C1) | 3/5 | AUROC +25‚Äì40%, P@1 +10‚Äì15% | ‚ùå Kh√¥ng |
| ü•à | **Session-Invariant Denoiser** (Phase C2) | 2/5 | EER -15‚Äì25%, AUROC +5‚Äì8% | ‚ùå Kh√¥ng |
| ü•â | **TTA BN Adaptation** (Phase A3) | 1/5 | AUROC +5‚Äì10%, EER -4‚Äì6% | ‚ùå Kh√¥ng |
| 4 | **Prototypical Inference** (ƒë√£ c√≥ code) | 1/5 | Open enrollment | ‚ùå Kh√¥ng |
| 5 | **Delta band focus** (Phase B4 ‚Äî fix frequency) | 2/5 | AUROC +8‚Äì12% | ‚ö†Ô∏è C√≥ nh∆∞ng sai bands |

### Promote Prototypical Inference (1 gi·ªù, ƒë√£ c√≥ code)

```python
# experiments/shared/trainer.py ‚Äî compute_centroids() ƒë√£ c√≥ t·∫°i d√≤ng ~347
# accuracy_centroid() ƒë√£ c√≥ t·∫°i d√≤ng ~72
# Ch·ªâ c·∫ßn ƒë·∫∑t ƒë√¢y l√†m PRIMARY inference path thay v√¨ secondary metric:

# Trong evaluate_comprehensive(), thay primary prediction b·∫±ng:
centroids = self.compute_centroids(model, train_dl, num_classes)
# Classify by nearest centroid ‚Üí prototypical inference
```

---

## Th·ª© T·ª± Implementation ƒê∆∞·ª£c ƒê·ªÅ Xu·∫•t

```
NGAY H√îM NAY (2‚Äì3 gi·ªù):
  ‚ú¶ A1: Fix ArcFace hardcode bug ‚Üí ch·∫°y l·∫°i ƒë·ªÉ confirm gain
  ‚ú¶ A5: Fix filter_low=0.5 (prerequisite cho delta band)
  ‚ú¶ A2: embed_dim=256
  ‚ú¶ A6: 5 seeds

TU·∫¶N 1:
  ‚ú¶ A3: TTA (highest AUROC ROI kh√¥ng c·∫ßn ki·∫øn tr√∫c m·ªõi)
  ‚ú¶ A4: Time shift + channel dropout

TU·∫¶N 2:
  ‚ú¶ B4: Delta band SincNet branch (AUROC ROI cao nh·∫•t Phase B)
  ‚ú¶ B1: Hybrid SupCon (sau B4 ƒë·ªÉ kh√¥ng bottlenecked by architecture)

TU·∫¶N 3:
  ‚ú¶ B5: Subject-balanced sampler
  ‚ú¶ B3: EEGNet 1D embedder
  ‚ú¶ B2: Joint fine-tune Stage 3

TU·∫¶N 4‚Äì6:
  ‚ú¶ C2: Session-invariant loss (data prep tr∆∞·ªõc)
  ‚ú¶ C1: EEGPT (t·∫£i weights, integrate)
  ‚ú¶ C3: Conformer embedder

KH√îNG N√äN L√ÄM:
  ‚úó PhysioNet pretraining (domain mismatch 64ch motor imagery vs 4ch resting)
  ‚úó Mixup augmentation (N=14 subjects, c√≥ th·ªÉ harmful)
  ‚úó Memory bank hard mining (kh√¥ng c√≥ ƒë·ªß subjects ƒë·ªÉ c√≥ t√°c d·ª•ng)
  ‚úó Hyperbolic embeddings (EEG biometrics kh√¥ng c√≥ c·∫•u tr√∫c hierarchy)
```

---

## Complete Change Table

| | Improvement | File | Expected P@1 | Expected AUROC | Expected EER | Effort |
|---|---|---|---|---|---|---|
| A1 | Fix ArcFace s=64/m=0.5 bug ‚ö° | `trainer.py:205`, `dp.py:83` | +2‚Äì4% | +4‚Äì7% | -3‚Äì5% | 2h |
| A2 | embed_dim 128‚Üí256 | `dp.py:75` | +1‚Äì2% | +1‚Äì2% | -1% | 30m |
| A3 | TTA BN adaptation ‚ö° | `trainer.py:~388` | +1‚Äì2% | +5‚Äì10% | -4‚Äì6% | 3h |
| A4 | Time shift + channel dropout | `trainer.py:130` | +1‚Äì3% | +2‚Äì3% | -2‚Äì3% | 4h |
| A5 | Fix filter_low=0.5 ‚ö° | `dp.py:55` | ‚Äî | prerequisite | ‚Äî | 5m |
| A6 | 5 seeds + CI95 | `main.py:11`, `pipeline.py:116` | ‚Äî | ‚Äî | ‚Äî | 1h |
| B1 | Hybrid ArcFace + SupCon | `trainer.py:204` | +1% | +12‚Äì18% | -8‚Äì12% | 2d |
| B2 | Joint fine-tune Stage 3 | `trainer.py:after 291` | +3‚Äì5% | +5‚Äì8% | -4‚Äì6% | 2d |
| B3 | EEGNet 1D embedder | `model.py:after 115` | +1‚Äì3% | +3‚Äì5% | -3‚Äì5% | 3d |
| B4 | Delta band SincNet branch | `model.py:after EEGNet` | +2‚Äì3% | +8‚Äì12% | -5‚Äì8% | 2d |
| B5 | Subject-balanced sampler | `pipeline.py:after 165` | +1‚Äì2% | +10‚Äì15% | -6‚Äì10% | 2d |
| C1 | EEGPT foundation model | new `eegpt_adapter.py` | +2‚Äì4% | +3‚Äì6% | -3‚Äì5% | 2w |
| C2 | Session-invariant denoiser loss | `trainer.py:new`, `dp.py:378` | +1‚Äì2% | +5‚Äì8% | -4‚Äì6% | 1w |
| C3 | EEG Conformer multi-scale | `model.py:after EEGNet` | +1‚Äì3% | +3‚Äì5% | -2‚Äì4% | 1w |

‚ö° = Quick win, implement ngay h√¥m nay

---

## Sources

- Deng et al. (2019). ArcFace: Additive Angular Margin Loss. CVPR. arxiv.org/abs/1801.07698
- Lawhern et al. (2018). EEGNet: A compact CNN for EEG-based BCIs. J Neural Eng. arxiv.org/abs/1611.08024
- Liu et al. (2024). EEGPT: Pretrained Transformers for EEG. NeurIPS. arxiv.org/abs/2401.12291
- Khosla et al. (2020). Supervised Contrastive Learning. NeurIPS. arxiv.org/abs/2004.11362
- Wang et al. (2021). Tent: Fully Test-Time Adaptation by Entropy Minimization. ICLR.
- Song et al. (2022). EEG Conformer. IEEE TNNLS. arxiv.org/abs/2010.00274
- Jiang et al. (2024). LaBraM: Large Brain Model. ICLR. openreview.net/forum?id=QzTpTRVtrP
- PMC9735871. EEG Biometric Identification on Raspberry Pi (BED dataset). pmc.ncbi.nlm.nih.gov
- MSGM Paper (2026). Multi-Scale Spatiotemporal Graph Mamba. Frontiers in Neuroscience.
- DAAE. Domain-Adaptive Autoencoder for EEG Biometrics. mdpi.com
- DCTAU. All Beings Are Equal in Open Set Recognition. arxiv.org
- AES-MBE (2026). 4-electrode EEG biometrics 98.82% accuracy. pmc.ncbi.nlm.nih.gov
